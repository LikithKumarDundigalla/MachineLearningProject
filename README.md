# Distinguishing Human and ChatGPT Text

## Overview
This repository contains code for a machine learning model that distinguishes between human-generated text and text generated by ChatGPT. It explores various Natural Language Processing (NLP) techniques and models to achieve this classification task.
## Table of Contents
- [Approach Overview](#approach-overview)
- [Code Structure](#code-structure)
- [Usage](#usage)
- [File Descriptions](#file-descriptions)

## Approach Overview
There are three main approaches used in this project:

### TF-IDF Approach
The code for the online approach is provided in the `online_approach.py` file. This approach leverages existing models and methodologies showcased in [Analytics Vidhya's article](https://www.analyticsvidhya.com/blog/2023/04/how-to-build-a-machine-learning-model-to-distinguish-if-its-human-or-chatgpt/).

### Ngrams Approach
The `Ngrams_approach.py` file implements an approach based on N-grams for feature extraction and classification. It uses [CountVectorizer](https://www.datacamp.com/blog/what-is-tokenization) to generate N-gram features and employs Multinomial Naive Bayes for classification.

### Word Embeddings Approach
The `WordEmbeddings_approach.py` file utilizes Word Embeddings through the Gensim library's Word2Vec model. It transforms text data and employs Logistic Regression for classification.

## Code Structure
The codebase is organized as follows:
-`main.py` : Initiation function, starts executing the whole project. 
- `online_approach.py`: Contains the code for the online approach.
- `Ngrams_approach.py`: Implements the N-grams based approach.
- `WordEmbeddings_approach.py`: Code for the Word Embeddings approach.
- `requirements.txt`: Lists the necessary libraries and their versions required to run the code.

## Usage
To use this repository:

1. Clone the repository: `git clone https://github.com/ml4ai-2023-fall-ml/final-project-option-b-LikithKumarDundigalla.git`
2. Navigate to the project directory: `cd final-project-option-b-LikithKumarDundigalla`
3. Install the required libraries: `pip install -r requirements.txt`
4. Run the main script: `python main.py`

## File Descriptions
- `dataset.py` : Extracts data from "Hugging Face" and saves as a csv file.
- `main.py`: Entry point for orchestrating different approaches. 
- `online_approach.py`: Implements the TF-IDF approach using existing methodologies.
- `Ngrams_approach.py`: Contains code for the N-grams based approach.
- `WordEmbeddings_approach.py`: Code for the Word Embeddings approach.
- `requirements.txt`: Lists required libraries and versions.
